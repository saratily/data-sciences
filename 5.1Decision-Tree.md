# Decision Tree
```
Overview
Components
Advantages and disadvantages
Predictor vs Classifier
Types of Classifier
    Linear
    NonLinear
    Decision Tree
Impurity Measures
    Clasification Tree
        1. Gini Index
        2. Entropy
        2a. Joint Entropy
        2b. Conditional Entropy
        3. Information Gain
    Regression Tree
        4. Variance
Overfitting
    Ensemble learning
    Bootstrap Aggregation (Bagging)
```

# Overview
```
    https://en.wikipedia.org/wiki/Decision_tree
    |_ Tree based model - help in making decision
    |_ can realize boolean functions
    |_ less misclassification as compare to any linear function - when it is difficult to have straight line for classification
    |_ used for both regression and classification problems
    |_ have hierarchical structure 
    |_ splits data into smaller subsets, based on condition (with binary or multiple options) A sub-node that splits into further sub-nodes
    |_ if tree depth is not set, then it will only stop when all the leaves are homogenous i.e.  impurity in every leaf is zero
```

## Components
- Root - Starting node
- Branch or Sub-Tree - part of decision tree
- Splitting - Dividing a node into two or more sub-nodes based on conditions
- Decision Node -  A sub-node that splits into further sub-nodes, all nodes except leaf node
- Leaf or Terminal Node - end of tree, can't divide further
- Depth of the tree - # of nodes, from root -> furthest leaf 

## Advantages and disadvantages
### Advantages
- Easy to understand - can visualize and understand the logic
- Verstie - can handle numerical and categorical data
- Powerful - can model arbitrary functions as long as we have sufficient data
- Requires little data preparation
- Performs well with large datasets
- Build-in feature selection
- Testable

### Disadvantages
- Can be non-robust
    |_ small change -> might be large impact (consequences on final result)
- computationally expensive
    |_ try all possible combinations at each split
- NP complete problems
    |_ number of possible outcomes: n!
    |_ donot have simple algo to solve these problems
    |_ Use greedy algorithm to solve these problems
- Always overfitting

## Building a Decision Tree
## Predictor vs Classifier
Predictor - fitting a shape that gets as close to the data as possible
Classifier - separating data into class

## Types of Classifier
### Linear
     |_ uses linear funtion, i.e. draw striaght line, to split data
### NonLinear
    |_ uses a non-linear function to split data 
### Decision Tree
    |_ Divides data into subsets 

## Error Type and Confusion Matrix
## Numerical Vs Categorical Data

## Impurity Measures
    |_ minimize impurity to find the best split for decision tree

### 1. Gini Index
    |_ For classification tree
    |_ Range = 0 - 0.5, meaning 0 -> most pure, 0.5 -> most impure
    |_ Non additive, aasy to compute
    |_ H(Z) = $\sum_{??}^{} ??(??)(1-??(??))$
            = 1- $\sum_{??}^{} ??(??)^2$

### 2. Entropy
    |_ For classification tree
    |_ Range = 0 - 1, meaning 0 -> most pure, 1 -> most impure
    |_ Additive, computationally intensive
    |_ measure of randomness or impurity contained in a dataset
    |_  maximum when both outcomesare equally likely, worst
        |_ perfectly uniform coin H: 0.5, T: 0.5 Entropy=1
    |_ minimum when the outcome is homogeneous
        |_ uneven coin: H: 0.9, T: 0.1, Entropy=0.14
    |_ lower entropy, higher purity
    |_ H(Z) = - $\sum_{(??,??)?????}^{} ??(??,??)??????(??(??,??))$
            = - plogp - (1-p)log(1-p)

### 2-a. Joint Entropy
    |_ Calculate entropy of 2 variables
    |_ ??(??,??)

### 2-b. Conditional Entropy
    |_. entropy of the target variable for a given split
    |_ H(Y|X)

### 3. Information Gain
    |_ For classification tree
    |_ goal is to maximize information gain, and minimize ??(??|??) [ H(Y) is constant]
    |_ in other words, maximize diff b/w H(Y) and H(Y|X)
    |_ Range = 0 - 1, meaning 0 -> less gain, 1 -> more gain
    |_ Computationally intensive
    |_ I??(??|??)=??(??)???(??|??)

### 4. Variance
    |_ For regression tree
    |_ The most common measure of dispersion    

### Greedy Algorithm

# Overfitting
    |_ occurs when a model has high complexity and captures both information and noise in the training data.
    |_ when a model performs good on training data but poor on test (unseen) data.

### Ensemble learning
    |_ for better Prediction
    |_ combining predictions from different models
    |_ works best when the learners are as independent as possible.
    |_ prob(majority_classifiers)  making a mistake is much < prob of any one of them making a mistake.

### Bootstrap Aggregation (Bagging)
    |_ decreases variance and thus helps with overfitting.
    |_ use multiple data samples, generated by random sampling with replacement, from the original dataset to create multiple models and aggregating their predictions
    |_ choose random subsamples of the data points with replacement.
    |_ merge the output of multiple models to get the final result
    |_ training can be done in parallel on each subset
    |_ subtrees are independent of each other
    |_ It uses averaging and majority voting to smooth-out predictions
    