**Glossary of Notation - Introduction to Supervised Learning and Classification**

<b>X<sub>i</sub></b> = Vector containing values of input features corresponding to the i<sup>tℎ</sup> record, where i ranges from 1 to n

<b>Y<sub>i</sub></b> = Value of the output variable corresponding to the i<sup>tℎ</sup> record

$\hat{Y}$ = The predicted value of the output variable

E = Expected value or average

ϕ(X) = A transformed version of feature vector X

ℎ(X) = Non-linear classiﬁer function

θ<sup>T</sup> = Transpose of vector θ

µ<sub>k</sub> = Mean vector for class k

C<sub>k</sub> = Covariance matrix for class k

π = Probability of a data point belonging to class k. These are called **prior probabilities**

N(µ<sub>k</sub> , C<sub>k</sub> ) = Normal distribution with mean µ and covariance C<sub>k</sub>

γ<sub>k</sub> = Normalizing constant for class k in the normal distribution equation

P(Y = k | X) = Probability of data point belonging to class 푘 given the input features X. These are called **posterior probabilities**

P(X | Y = k) = Probability of X given the output class Y = k

C = When covariances of all the classes are the same

C<sub>def</sub> = The cost of someone being a defaulter

C<sub>lost</sub> = The cost of losing a customer

$\hat{π}$<sub>k</sub> = Estimate of π<sub>k</sub>

$\hat{µ}$<sub>k</sub> = Estimate of µ<sub>k</sub>

$\hat{C}$<sub>k</sub> = Estimate of C<sub>k</sub>

L(data; θ) = Likelihood function of the observed data

γ = Regularization hyperparameter

**w** = Weight in the likelihood equation for unbalanced data

||x|| = Distance of a point x from the origin

w = Weight in the weighted distance metric
