**Decision Tree - Glossary of Notations**

𝒳 = A vector of categorical data

ｙ = Outcome class (categorical)

𝑓: 𝒳 →ｙ = Decision Rule, i.e., 𝑓 is a function that is mapping the independent features to the target values

𝑥𝑖 = 𝑖𝑡ℎ row of the vector 𝒳

𝑦𝑖 = 𝑖𝑡ℎ row of vectorｙ

N = Natural number

∈ = Belongs to

Σ = The summation

≠ = Not equal to

𝑅(𝑓) = Empirical Error (generalization error) of a Decision Rule

𝑅 * (𝑓) = Probabilistic Error of a Decision Rule

      𝑁
(1/𝑁)∑  𝐼 (𝑓(𝑥𝑖 ) ≠ 𝑦𝑖 ) = The average number of misclassifications. The () function is 1 in case of a misclassification and 0 otherwise
      𝑖
𝐶 = It is a subclass of data points

𝑘 = Subset of all feature indices in the subclass

𝑍 = Random Variable

𝑋, 𝑌 = 𝑋 represent the independent features and 𝑌 represents the target feature

𝑃(𝑍) = Probability mass function of the random variable 𝑍

𝐸 = Expected value

𝑃(𝑥, 𝑦) = It represents the joint distribution of X and Y

𝐻(𝑍) = Entropy of 𝑍

𝐻(𝑋, 𝑌) = Joint Entropy of random variables X and Y

𝐻(𝑋 | 𝑌) = Conditional Entropy of X given Y

𝐼𝐺(𝑌 | 𝑋) = Information Gain of Y given X

𝑋 ⊥ 𝑌  = 𝑋  is perpendicular to 𝑌

𝑋(𝑚) = A feature from the 𝑋

𝑆1 = = {(𝑦𝑖 |𝑥𝑖 (𝑚)= 0} = Splitting outcome based on class 0

𝑆2 = = {(𝑦𝑖 |𝑥𝑖 (𝑚)= 1} = Splitting outcome based on class 1



